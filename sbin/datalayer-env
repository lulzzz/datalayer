#!/usr/bin/env bash

# Licensed to Datalayer (http://datalayer.io) under one or more
# contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership. Datalayer licenses this file
# to you under the Apache License, Version 2.0 (the 
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

export DLABIN="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"

# ------------------------------------

function errexit() {
  local err=$?
  set +o xtrace
  local code="${1:-1}"
  echo "Error in ${BASH_SOURCE[1]}:${BASH_LINENO[0]}. '${BASH_COMMAND}' exited with status $err"
  # Print out the stack trace described by $function_stack  
  if [ ${#FUNCNAME[@]} -gt 2 ]
  then
    echo "Call tree:"
    for ((i=1;i<${#FUNCNAME[@]}-1;i++))
    do
      echo " $i: ${BASH_SOURCE[$i+1]}:${BASH_LINENO[$i]} ${FUNCNAME[$i]}(...)"
    done
  fi
  echo "Exiting with status ${code}"
  exit "${code}"
}

# trap ERR to provide an error handler whenever a command exits nonzero
#  this is a more verbose version of set -o errexit
trap 'errexit' ERR

# setting errtrace allows our ERR trap handler to be propagated to functions,
#  expansions and subshells
# set -o errtrace

# ------------------------------------

if [ -z "$DLAHOME" ]
then
  export DLAHOME=$DLABIN/..
fi
export PATH=$DLAHOME/bin:$PATH

# ------------------------------------

export DATALAYER_VERSION=1.0.0

if [ -z "$DLA_OPT" ]
then
  export DLA_OPT=/opt
fi

export DLA_ECHO_SEPARATOR="""─────────────────────────────────────────────────────────────────────"""

if [ -z "$DLA_SRC" ]
then
  export DLA_SRC=/src
fi

# ------------------------------------

if [ -z "$DLA_DOWNLOAD_EXT_SKIP" ]
then
  export DLA_DOWNLOAD_EXT_SKIP="false"
fi

if [ -z "$DLA_OPT_FOLDER" ]
then
  export DLA_OPT_FOLDER=/opt
fi

# ------------------------------------

export DLA_CLASSPATH=$DLAHOME/lib/*:$DLA_CLASSPATH

export DLA_CLASSPATH_FILE=$DLAHOME/gen/datalayer-classpath.sh

# ------------------------------------

if [ -z "$DLA_REPOSITORY" ]
then
  export DLA_REPOSITORY=$DLAHOME/mvn/repository
fi

# ------------------------------------

if [ -z "$DLA_ENV_ECHO" ]
then
  export DLA_ENV_ECHO=false
fi

# ------------------------------------

if [ -z "DLA_SHOW_HEADER" ]
then
  export DLA_SHOW_HEADER=true
fi

# ------------------------------------

if [ -z "DLA_DOWNLOAD_EXT_SKIP" ]
then
  export DLA_DOWNLOAD_EXT_SKIP=true
fi

# ------------------------------------

export JAVA_HOME=$DLA_OPT/jdk
export PATH=$JAVA_HOME/bin:$PATH
unset JAVA_TOOL_OPTIONS

if [ -z "$JAVA_HOME" ]
then
  if [ -e "/opt/jdk/jre/bin/java" ]
  then
    export JAVA_HOME="/opt/jdk"
  else
    echo
    echo -e "!!! No JAVA_HOME found here... !!!"
    echo -e "    Set JAVA_HOME e.g. to /opt/jdk-1.8.0"
    echo
    exit -1
  fi
fi

# ------------------------------------

source $DLAHOME/sbin/datalayer-cli-control

# ------------------------------------

export MAVEN_HOME=$DLA_OPT_FOLDER/apache-maven-3.3.9
export PATH=$MAVEN_HOME/bin:$PATH
export MAVEN_OPTS=-Xmx1g

if [ ! -f $MAVEN_HOME/bin/mvn ]; then
  datalayer install-ext -c maven
fi

# ------------------------------------

if [ -z "$DLA_PORT" ]
then
  ZEPPELIN_PORT=
else
  export ZEPPELIN_PORT=$DLA_PORT
fi

# ------------------------------------

if [ -z "$DLA_HDP" ]; then

  if [ ! -f $DLA_OPT_FOLDER/hadoop-2.9.0/bin/hadoop ]; then
    datalayer install-ext -c hadoop
  fi

fi

# ------------------------------------

export SPARK_HOME=$DLA_OPT_FOLDER/spark
export PATH=$SPARK_HOME/bin:$PATH

# if [ ! -f $SPARK_HOME/bin/spark-shell ]; then
#  datalayer install-ext -c spark
# fi

export PYTHONPATH=$SPARK_HOME/python/:$PYTHONPATH
export PYTHONPATH=$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$PYTHONPATH

export SPARK_YARN_USER_ENV="PYTHONPATH=${PYTHONPATH}"

# ------------------------------------

export ZOOKEEPER_HOME=$DLA_OPT_FOLDER/zookeeper
export PATH=$ZOOKEEPER_HOME/bin:$PATH

if [ -z "$ZOOKEEPER_CONF_FILE" ]; then
  export ZOOKEEPER_CONF_FILE=$DLAHOME/conf/zookeeper/zoo.cfg
fi

if [ ! -f $ZOOKEEPER_HOME/bin/zkServer.sh ]; then
  datalayer install-ext -c zookeeper
fi

# ------------------------------------

export ADS_HOME=$DLA_OPT_FOLDER/apacheds-2.0.0-M20
export PATH=$ADS_HOME/bin:$PATH

if [ ! -f $ADS_HOME/bin/apacheds.sh ]; then
  datalayer install-ext -c apacheds
fi

# ------------------------------------

export KAFKA_HOME=$DLA_OPT_FOLDER/kafka_2.11-0.8.2.1
export PATH=$KAFKA_HOME/bin:$PATH

if [ ! -f $KAFKA_HOME/bin/kafka-server-start.sh ]; then
  datalayer install-ext -c kafka
fi

# ------------------------------------

export GREMLIN_CONSOLE_HOME=/opt/apache-tinkerpop-gremlin-console-3.2.4
export PATH=$GREMLIN_CONSOLE_HOME/bin:$PATH

# if [ ! -f $GREMLIN_CONSOLE_HOME/bin/ ]; then
#   datalayer install-ext -c gremlin-console
# fi

export GREMLIN_SERVER_HOME=/opt/apache-tinkerpop-gremlin-server-3.2.4
export PATH=$GREMLIN_SERVER_HOME/bin:$PATH

# if [ ! -f $GREMLIN_CONSOLE_HOME/bin/ ]; then
#   datalayer install-ext -c gremlin-console
# fi

# ------------------------------------

export TITAN_HOME=/opt/titan-1.1.0-SNAPSHOT-hadoop2
export PATH=$TITAN_HOME/bin:$PATH

# if [ ! -f $TITAN_HOME/bin/titan.sh ]; then
#   datalayer install-ext -c neo4j
# fi

# ------------------------------------

export NEO4J_HOME=$DLA_OPT_FOLDER/neo4j-community-3.1.0
export PATH=$NEO4J_HOME/bin:$PATH

if [ ! -f $NEO4J_HOME/bin/neo4j ]; then
  datalayer install-ext -c neo4j
fi

# ------------------------------------

export ELASTICSEARCH_HOME=$DLA_OPT_FOLDER/elasticsearch
export PATH=$ELASTICSEARCH_HOME/bin:$PATH

# if [ ! -f $ELASTICSEARCH_HOME/bin/elasticsearch ]; then
#   datalayer install-ext -c elasticsearch
# fi

# ------------------------------------

export SOLR_HOME=$DLA_OPT_FOLDER/solr
export PATH=$SOLR_HOME/bin:$PATH

# if [ ! -f $SOLR_HOME/bin/solr ]; then
#   datalayer install-ext -c solr
# fi

# ------------------------------------

export CASSANDRA_HOME=$DLA_OPT_FOLDER/apache-cassandra-3.7
export PATH=$CASSANDRA_HOME/bin:$PATH

if [ ! -f $CASSANDRA_HOME/bin/cassandra ]; then
  datalayer install-ext -c cassandra
fi

# ------------------------------------

export PHOENIX_HOME=$DLA_OPT_FOLDER/phoenix-4.10.0-HBase-1.2-SNAPSHOT
export PATH=$PHOENIX_HOME/bin:$PATH

# if [ ! -f $PHOENIX_HOME/bin/sqlline.py ]; then
#   datalayer install-ext -c phoenix
# fi

# ------------------------------------

if [ -z "$HIVE_HOME" ]; then
  export HIVE_HOME=/opt/apache-hive-2.3.0-bin
fi
export PATH=$HIVE_HOME/bin:$PATH

# ------------------------------------

if [ -z "$HBASE_HOME" ]; then
#  export HBASE_HOME=/opt/hbase-1.2.0
  export HBASE_HOME=/opt/hbase-2.0.0-SNAPSHOT
fi
export PATH=$HBASE_HOME/bin:$PATH
if [ -z "$HBASE_CONF_DIR" ]; then
  export HBASE_CONF_DIR=$DLAHOME/conf/hbase
fi

# Tell HBase whether it should manage its own instance of Zookeeper or not.
# HBASE_MANAGES_ZK variable in conf/hbase-env.sh has to be set to false (This tells hbase not to start its own zookeeper ensemble)
export HBASE_MANAGES_ZK=false

if [ ! -f $HBASE_HOME/bin/hbase ]; then
  datalayer install-ext -c hbase
fi

# ------------------------------------

if [ -z "$DLA_NOTEBOOK_DIR" ]
then
  export ZEPPELIN_NOTEBOOK_DIR=$DLAHOME/notebooks/school
else
  export ZEPPELIN_NOTEBOOK_DIR=$DLA_NOTEBOOK_DIR
fi

export ZEPPELIN_IDENT_STRING=$USER

# ------------------------------------

if [ -z "$DLA_HADOOP" ]
then
  # Run by default without hadoop
  export DLA_HADOOP=none
fi

# Define the way the notebook connects to SPARK (you must have SPARK_HOME defined to your spark local distribution)
if [ "$DLA_HADOOP" == "yarn" ]
then
  export MASTER=yarn
  export SPARK_YARN_MODE=true
  if [ "$DLA_HADOOP_STATUS" != "started" ]
  then
    if [ -z "$DLA_HADOOP_DATA_DIR" ]
    then
      export DLA_HADOOP_DATA_DIR=$DLAHOME/var/datalayer/hadoop
    fi
    if [ -z "$HADOOP_CONF_DIR" ]
    then
      export HADOOP_CONF_DIR=$DLAHOME/conf/hadoop
      if [ "$DLA_SECURITY" == "kerberos" ]
      then
        export DLA_HADOOP_CONF_TEMPLATE_DIR=$DLAHOME/conf/hadoop-kerberos
      else
        export DLA_HADOOP_CONF_TEMPLATE_DIR=$DLAHOME/conf-template/hadoop-simple
      fi
      datalayer hadoop-conf-generate
    fi
    if [ -z "$HADOOP_HOME" ]
    then
      if [ "$DLA_SECURITY" == "kerberos" ]
      then
      #  For kerberos security, user is supposed to preset HADOOP_HOME with a root owned Hadoop distribution.
        export HADOOP_HOME=/hadoop-2.9.0-secure
      else
        export HADOOP_HOME=$DLA_OPT_FOLDER/hadoop-2.9.0
      fi
    fi
    export PATH=$HADOOP_HOME/bin:$PATH
    if [ ! -d $DLA_HADOOP_DATA_DIR ]; then
      mkdir -p $DLA_HADOOP_DATA_DIR
      export DLA_HADOOP_STATUS=init
      $DLAHOME/bin/datalayer hadoop-init
      export DLA_HADOOP_STATUS=stopped
    fi
    if [ "$DLA_HADOOP_STATUS" != "stopped" ]
    then
      if [ "$DLA_HADOOP_STATUS" != "started" ]
      then
        if [ "$DLA_HADOOP_STATUS" != "init" ]
        then
          if [ "$DLA_HADOOP_STATUS" != "stopping" ]
          then
            if [ "$DLA_HADOOP_STATUS" != "starting" ]
            then
              export DLA_HADOOP_STATUS=starting
              $DLAHOME/bin/datalayer hadoop-start
              export DLA_HADOOP_STATUS=started
            fi
          fi
        fi
      fi
    fi
  fi
else
  export MASTER=local[*]
  export HADOOP_HOME=
  export HADOOP_CONF_DIR=
  if [ -z "$SPARK_DAEMON_MEMORY" ]
  then
    export SPARK_DAEMON_MEMORY=1g
  fi
fi
