#!/usr/bin/env bash

# Licensed to Datalayer (http://datalayer.io) under one or more
# contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership. Datalayer licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

# Once you have started a worker, look at the masterâ€™s web UI (http://localhost:8080 by default). 
# You should see the new node listed there, along with its number of CPUs and memory (minus one gigabyte left for the OS).

export DLABIN="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
source $DLABIN/util/datalayer-env

source $DLAHOME/sbin/datalayer-cli-control

echo
echo "Check Spark Application      http://localhost:4040"
echo "Check Spark History Server   http://localhost:18080"
echo
echo "Other options:"
echo "--------------"
echo
echo "MASTER=yarn HADOOP_CONF_DIR=$DLA_HOME/conf/hadoop $SPARK_HOME/bin/spark-shell --packages javax.mail:mail:1.4.5-rc1"
echo """
Users may also include any other dependencies by supplying a comma-delimited list of maven coordinates with --packages.
All transitive dependencies will be handled when using this command. Additional repositories (or resolvers in SBT) can be added in a comma-delimited fashion with the flag --repositories.
(Note that credentials for password-protected repositories can be supplied in some cases in the repository URI, such as in https://user:password@host/.... Be careful when supplying credentials this way.).

These commands can be used with pyspark, spark-shell, and spark-submit to include Spark Packages.

For Python, the equivalent --py-files option can be used to distribute .egg, .zip and .py libraries to executors.
"""
